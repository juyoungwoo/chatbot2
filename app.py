import streamlit as st
import os
import io
from langchain_community.chat_models import ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from google.oauth2 import service_account
from googleapiclient.discovery import build
import tempfile

# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# Google Drive ì„¤ì •
@st.cache_resource
def init_drive_service():
    credentials = service_account.Credentials.from_service_account_info(
        st.secrets["google_credentials"],
        scopes=['https://www.googleapis.com/auth/drive.readonly']
    )
    return build('drive', 'v3', credentials=credentials)

# PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
def get_pdf_files(service, folder_id):
    results = service.files().list(
        q=f"'{folder_id}' in parents and mimeType='application/pdf'",
        fields="files(id, name)"
    ).execute()
    return results.get('files', [])

# Streamlit UI êµ¬ì„±
st.title("ğŸ“„ IPRì‹¤ ë§¤ë‰´ì–¼ AI ì±—ë´‡")
st.write("ì¶”ê°€ì ì¸ ìë£Œ ì—…ë°ì´íŠ¸ í¬ë§ì‹œ ì£¼ì˜ ì—°êµ¬ì› ìš”ì²­")

try:
    # ë“œë¼ì´ë¸Œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    service = init_drive_service()
    FOLDER_ID = '1fThzSsDTeZA6Zs1VLGNPp6PejJJVydra'  # êµ¬ê¸€ ë“œë¼ì´ë¸Œ í´ë” ID
    
    # PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    pdf_files = get_pdf_files(service, FOLDER_ID)
    
    if not pdf_files:
        st.warning("í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info(f"ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ë¶„ì„í•©ë‹ˆë‹¤.")
        
        # ëª¨ë“  PDF íŒŒì¼ì˜ ë‚´ìš©ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        @st.cache_resource
        def process_all_pdfs():
            all_texts = []
            for pdf in pdf_files:
                try:
                    # PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                    request = service.files().get_media(fileId=pdf['id'])
                    file_content = request.execute()
                    
                    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
                        temp_file.write(file_content)
                        pdf_path = temp_file.name
                    
                    # PDF ë¡œë“œ ë° ì²˜ë¦¬
                    loader = PyPDFLoader(pdf_path)
                    documents = loader.load()
                    
                    # íŒŒì¼ ì´ë¦„ì„ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                    for doc in documents:
                        doc.metadata['source'] = pdf['name']
                    
                    all_texts.extend(documents)
                    
                    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                    os.unlink(pdf_path)
                    
                except Exception as e:
                    st.warning(f"{pdf['name']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            
            # ë¬¸ì„œ ë¶„í• 
            text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
            split_texts = text_splitter.split_documents(all_texts)
            
            # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
            embeddings = OpenAIEmbeddings()
            vector_store = FAISS.from_documents(split_texts, embeddings)
            
            return vector_store
        
        # ëª¨ë“  PDF ì²˜ë¦¬
        vector_store = process_all_pdfs()
        retriever = vector_store.as_retriever(search_kwargs={"k": 2})
        
        # í”„ë¡¬í”„íŠ¸ ì„¤ì •
        system_template = """
        Use the following pieces of context to answer the users question shortly.
        Given the following summaries of a long document and a question.
        If you don't know the answer, just say that "I don't know", don't try to make up an answer.
        If possible, mention which document (source) the information comes from.
        ----------------
        {summaries}
        You MUST answer in Korean and in Markdown format:
        """
        messages = [
            SystemMessagePromptTemplate.from_template(system_template),
            HumanMessagePromptTemplate.from_template("{question}")
        ]
        prompt = ChatPromptTemplate.from_messages(messages)
        chain_type_kwargs = {"prompt": prompt}
        
        # LLM ëª¨ë¸ ì„¤ì •
        llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
        
        # QA ì²´ì¸ ì„¤ì •
        chain = RetrievalQAWithSourcesChain.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs=chain_type_kwargs
        )
        
        # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
        query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
        if st.button("ì§ˆë¬¸í•˜ê¸°") and query:
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                result = chain({"question": query}, return_only_outputs=True)
                answer = result["answer"]
                st.markdown(answer)

except Exception as e:
    st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
