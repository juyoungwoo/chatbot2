# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hWko_g4_GQvCqvJ0MjDMFi5Gq1Ex9Mx3
"""

import streamlit as st
import os
from langchain_community.chat_models import ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# Streamlit UI êµ¬ì„±
st.title("ğŸ“„ PDF ê¸°ë°˜ AI ì±—ë´‡")
st.write("PDF ë‚´ìš©ì„ í•™ìŠµí•œ AI ì±—ë´‡ì…ë‹ˆë‹¤.")

# PDF ì—…ë¡œë“œ ê¸°ëŠ¥
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf")

if uploaded_file is not None:
    # PDF íŒŒì¼ ì €ì¥
    pdf_path = f"./{uploaded_file.name}"
    with open(pdf_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    # PDF ë¡œë“œ
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()

    # ë¬¸ì„œ ë¶„í• 
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    texts = text_splitter.split_documents(documents)

    # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    embeddings = OpenAIEmbeddings()
    vector_store = Chroma.from_documents(texts, embeddings)
    retriever = vector_store.as_retriever(search_kwargs={"k": 2})

    # í”„ë¡¬í”„íŠ¸ ì„¤ì •
    system_template = """
    Use the following pieces of context to answer the users question shortly.
    Given the following summaries of a long document and a question.
    If you don't know the answer, just say that "I don't know", don't try to make up an answer.
    ----------------
    {summaries}

    You MUST answer in Korean and in Markdown format:
    """
    messages = [
        SystemMessagePromptTemplate.from_template(system_template),
        HumanMessagePromptTemplate.from_template("{question}")
    ]
    prompt = ChatPromptTemplate.from_messages(messages)
    chain_type_kwargs = {"prompt": prompt}

    # LLM ëª¨ë¸ ì„¤ì •
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

    # QA ì²´ì¸ ì„¤ì •
    chain = RetrievalQAWithSourcesChain.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs=chain_type_kwargs
    )

    # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
    query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

    if st.button("ì§ˆë¬¸í•˜ê¸°") and query:
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            result = chain({"question": query}, return_only_outputs=True)
            answer = result["answer"]
            st.markdown(answer)